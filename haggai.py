# -*- coding: utf-8 -*-
"""Haggai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bMwrfxV2G3Fe7KnriwmBLRL6Br2kOFG9
"""

!pip install optuna

!pip install catboost

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import optuna
import lightgbm as lgb

# Scikit-Learn estimators
from sklearn.ensemble import RandomForestClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import KFold, cross_val_score
from xgboost import XGBRegressor
from catboost import CatBoostRegressor

# Model Evaluations
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score # Separate lines to keep everyting "Pythonic"
from sklearn.metrics import roc_curve
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# To handle warnings
import warnings

from google.colab import files
uploaded = files.upload()

dataset = pd.read_csv('starbucks_reviews_data.csv')

dataset

dataset.info()

dataset.isnull().sum()

dataset.describe()

mean_rating = dataset['Rating'].mean()
dataset['Rating'].fillna(mean_rating, inplace=True)

dataset.isnull().sum()

dataset.columns

dataset["Rating"]=dataset["Rating"].astype(int)

dataset["Rating"].value_counts()

plt.figure(figsize=(10,8))
sns.countplot(y="Rating",data=dataset,palette="cool")
plt.show()

rating_counts = dataset["Rating"].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Ratings')
plt.axis('equal')
plt.show()

dataset["location"].value_counts()

plt.figure(figsize=(20,18))
sns.countplot(y="location",data=dataset[:50],palette="cool")
plt.show()

"""Data pre-processing and visualization of some of the selected features.."""

dataset.drop(columns=["name","location","Date","Image_Links"],axis=1,inplace=True)

dataset.head()

def map_sentiment(score):
    if score in [1, 2]:
        return 'Negative'
    elif score == 3:
        return 'Neutral'
    elif score in [4, 5]:
        return 'Positive'
    else:
        return 'Unknown'
dataset['Rating'] = dataset.Rating.apply(map_sentiment)

dataset.rename(columns={"Rating":"label"},inplace=True)
dataset.rename(columns={"Review":"text"},inplace=True)

dataset["label"].value_counts()

label_counts = dataset["label"].value_counts()

plt.figure(figsize=(10,8))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=90, pctdistance=0.85)
plt.title('Distribution of Labels')
plt.gca().add_artist(plt.Circle((0,0),0.70,fc='white'))
plt.axis('equal')
plt.show()

plt.figure(figsize=(10,8))
sns.countplot(x="label",data=dataset,palette="hot")
plt.show()

from wordcloud import WordCloud, STOPWORDS

plt.figure(figsize=(15,15))
negative_wordcloud=dataset[dataset["label"]=="Negative"]
negative_text=" ".join(negative_wordcloud['text'].values.tolist())
wordcloud = WordCloud(width=800, height=800,stopwords=STOPWORDS, background_color='black', max_words=800,colormap="spring",).generate(negative_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

plt.figure(figsize=(15,15))
positive_wordcloud=dataset[dataset["label"]=="Positive"]
positive_text=" ".join(positive_wordcloud['text'].values.tolist())
wordcloud = WordCloud(width=800, height=800,stopwords=STOPWORDS, background_color='black', max_words=800,colormap="hot",).generate(positive_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

plt.figure(figsize=(15,15))
neutral_wordcloud=dataset[dataset["label"]=="Neutral"]
neutral_text=" ".join(neutral_wordcloud['text'].values.tolist())
wordcloud = WordCloud(width=800, height=800,stopwords=STOPWORDS, background_color='black', max_words=800,colormap="summer",).generate(neutral_text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

from collections import Counter

positive_text = dataset[dataset["label"] == "Positive"]
data_set = positive_text["text"].str.split()
all_words = [word for sublist in data_set for word in sublist]
counter = Counter(all_words)
common_words = counter.most_common(30)
dataset_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])
plt.figure(figsize=(12, 8))
sns.barplot(x='Count', y='Word', data=dataset_common_words,palette="gist_stern")
plt.title('30 Most Common Words Positive Words')
plt.xlabel('Count Positive Words')
plt.ylabel('Positive Words')
plt.show()

negative_text = dataset[dataset["label"] == "Negative"]
data_set = negative_text["text"].str.split()
all_words = [word for sublist in data_set for word in sublist]
counter = Counter(all_words)
common_words = counter.most_common(30)
dataset_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])
plt.figure(figsize=(12, 8))
sns.barplot(x='Count', y='Word', data=dataset_common_words,palette="gist_ncar")
plt.title('30 Most Common Words Negative Words')
plt.xlabel('Count Negative Words')
plt.ylabel('Negative Words')
plt.show()

neutral_text = dataset[dataset["label"] == "Neutral"]
data_set = neutral_text["text"].str.split()
all_words = [word for sublist in data_set for word in sublist]
counter = Counter(all_words)
common_words = counter.most_common(30)
dataset_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])
plt.figure(figsize=(12, 8))
sns.barplot(x='Count', y='Word', data=dataset_common_words,palette="cubehelix")
plt.title('30 Most Common Words Neutral Words')
plt.xlabel('Count Neutral Words')
plt.ylabel('Neutral Words')
plt.show()

dataset["text_length"]=dataset["text"].apply(len)

plt.figure(figsize=(10,8))
sns.histplot(dataset["text_length"], bins=30, kde=True,color="indigo")
plt.title('Distribution of Text Length')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.show()

avg_len=dataset["text"].apply(len)
avg_len=avg_len.mean()
print(f"Average Text Length is : {avg_len:.2f}")

dataset.drop(columns=["text_length"],axis=1,inplace=True)

dataset_positive=dataset[dataset["label"]=="Positive"]["text"].str.len()
dataset_negative=dataset[dataset["label"]=="Negative"]["text"].str.len()
dataset_neutral=dataset[dataset["label"]=="Neutral"]["text"].str.len()
plt.figure(figsize=(10, 6))
sns.histplot(dataset_positive, color='blue', alpha=0.5, label='Positive', kde=True)
sns.histplot(dataset_negative, color='red', alpha=0.5, label='Negative', kde=True)
sns.histplot(dataset_neutral, color='green', alpha=0.5, label='Neutral', kde=True)

plt.title('Distribution of Text Length by Label')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.legend()
plt.show()

from transformers import RobertaTokenizer, RobertaForSequenceClassification
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
max_len=400
sample_text=dataset["text"].iloc[0]
tokenize_data= tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True, max_length=max_len)
print("=======================================================================================================")
print("\n")
print(f"Orginal Text : {sample_text}")
print("\n")
print("=======================================================================================================")
print("=======================================================================================================")
print("=======================================================================================================")
print("\n")
print(f"Encoded Text : {tokenize_data}")

from sklearn.model_selection import train_test_split
dataset_train, dataset_test = train_test_split(dataset, test_size=0.2, random_state=42)
dataset_val, dataset_test = train_test_split(dataset_test, test_size=0.5, random_state=42)

from collections import Counter
from imblearn.over_sampling import RandomOverSampler
class_distribution=Counter(dataset_train["label"])
print(f"Class Distribution Before Random Oversampling : {class_distribution}")

dataset_train.head()

X=dataset_train.drop(columns="label",axis=1)
Y=dataset_train["label"]

print("Shapes of X and Y before oversampling:")
print("X shape:", X.shape)
print("Y shape:", Y.shape)

os=RandomOverSampler(random_state=42)
X_new,Y_new = os.fit_resample(X,Y)
dataset_train_resample = pd.concat([pd.DataFrame(X_new, columns=X.columns), pd.Series(Y_new, name="label")], axis=1)
dataset_train_resample.head()

dataset_train_resample.shape

class_count_after=Counter(dataset_train_resample["label"])
print(f"After Class Distribution Random OverSampling : {class_count_after}")

num_labels = len(set(dataset['label'].tolist()))
model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

num_labels

def tokenize_text(text):
    tokens = tokenizer.encode(text, add_special_tokens=True, padding=True, truncation=True, max_length=512)
    return tokens

dataset_train_resample["tokenize_text"]=dataset_train_resample["text"].apply(tokenize_text)
dataset_val["tokenize_text"]=dataset_val["text"].apply(tokenize_text)
dataset_test["tokenize_text"]=dataset_test["text"].apply(tokenize_text)

dataset_train_resample.head()

dataset_val.head()

dataset_test.head()

dataset_test.shape

from torch.nn.utils.rnn import pad_sequence
import torch

# Convert labels to numerical values (assuming they are categorical)
label_mapping = {label: i for i, label in enumerate(dataset_train_resample['label'].unique())}
dataset_train_resample['label'] = dataset_train_resample['label'].map(label_mapping)
dataset_val['label'] = dataset_val['label'].map(label_mapping)
dataset_test['label'] = dataset_test['label'].map(label_mapping)

X_train = pad_sequence([torch.tensor(seq) for seq in dataset_train_resample["tokenize_text"]], batch_first=True)

# ... (your previous code)

# Extract and convert test labels to a PyTorch tensor
Y_test = torch.tensor(dataset_test['label'].values)

Y_test

from torch.utils.data import TensorDataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch

# Assuming 'dataset_train_resample' is available from previous code
# Extract and convert training labels to a PyTorch tensor
Y_train = torch.tensor(dataset_train_resample['label'].values)

# Calculate padded sequences for validation data (assuming 'dataset_val' is available)
X_val = pad_sequence([torch.tensor(seq) for seq in dataset_val["tokenize_text"]], batch_first=True)

# Extract and convert validation labels to a PyTorch tensor (assuming 'dataset_val' is available)
Y_val = torch.tensor(dataset_val['label'].values)

# ... (your existing code to define X_train)

# Calculate padded sequences for test data (assuming 'dataset_test' is available)
X_test = pad_sequence([torch.tensor(seq) for seq in dataset_test["tokenize_text"]], batch_first=True) # Define X_test

# Extract and convert test labels to a PyTorch tensor
Y_test = torch.tensor(dataset_test['label'].values)

train_data=TensorDataset(X_train,Y_train)
train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)
val_data=TensorDataset(X_val,Y_val) # Now X_val is defined
val_dataloader = DataLoader(val_data, batch_size=8, shuffle=True)
test_data=TensorDataset(X_test,Y_test) # Now X_test is defined
test_dataloader = DataLoader(test_data, batch_size=8, shuffle=True)

import torch.nn as nn
from tqdm import tqdm

# Define the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)
criterion = nn.CrossEntropyLoss().to(device)

def train(model,dataloader,optimizer,criterion):
    model.train()
    train_loss=0.0
    for batch in tqdm(dataloader,desc="Training"):
        inputs,labels=batch
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs, labels=labels)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss/len(dataloader)

import torch.nn as nn
from tqdm import tqdm

model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.00001)
criterion = nn.CrossEntropyLoss().to(device)

def calculate_accuracy(outputs, labels):
    _, preds = torch.max(outputs, dim=1)
    corrects = torch.sum(preds == labels)
    accuracy = corrects.double() / len(labels)
    return accuracy.item()

def train(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0.0
    total_accuracy = 0.0
    for batch in tqdm(dataloader, desc="Training"):
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)  # Corrected this line
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        accuracy = calculate_accuracy(outputs.logits, labels)
        total_accuracy += accuracy
    avg_loss = total_loss / len(dataloader)
    avg_accuracy = total_accuracy / len(dataloader)
    return avg_loss, avg_accuracy

def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0.0
    total_accuracy = 0.0
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validation"):
            inputs, labels = batch
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs.logits, labels)
            total_loss += loss.item()
            accuracy = calculate_accuracy(outputs.logits, labels)
            total_accuracy += accuracy
    avg_loss = total_loss / len(dataloader)
    avg_accuracy = total_accuracy / len(dataloader)
    return avg_loss, avg_accuracy

train_loss_list = []
val_loss_list = []
train_accuracy_list = []
val_accuracy_list = []

epochs = 10
for epoch in range(epochs):
    avg_train_loss, avg_train_accuracy = train(model, train_dataloader, optimizer, criterion)
    train_loss_list.append(avg_train_loss)
    train_accuracy_list.append(avg_train_accuracy)

    avg_val_loss, avg_val_accuracy = evaluate(model, val_dataloader, criterion)
    val_loss_list.append(avg_val_loss)
    val_accuracy_list.append(avg_val_accuracy)

    print(f"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}")
    print(f"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f}")